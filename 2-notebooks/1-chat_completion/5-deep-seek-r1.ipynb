{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** is a state-of-the-art reasoning model combining reinforcement learning and supervised fine-tuning, excelling at complex reasoning tasks with 37B active parameters and 128K context window.\n",
        "\n",
        "In this notebook, you'll learn to:\n",
        "1. **Initialize** the ChatCompletionsClient for Azure serverless endpoints\n",
        "2. **Chat** with DeepSeek-R1 using reasoning extraction\n",
        "3. **Implement** a travel planning example with step-by-step reasoning\n",
        "4. **Leverage** the 128K context window for complex scenarios\n",
        "\n",
        "## Why DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Specializes in chain-of-thought problem solving\n",
        "- **Massive Context**: 128K token window for detailed analysis\n",
        "- **Efficient Architecture**: 37B active parameters from 671B total\n",
        "- **Safety Integrated**: Built-in content filtering capabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Required packages:\n",
        "- `azure-ai-inference`: For chat completions\n",
        "- `python-dotenv`: For environment variables\n",
        "\n",
        ".env file requirements:\n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "AZURE_INFERENCE_KEY=<your-api-key>\n",
        "MODEL_NAME=DeepSeek-R1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Client initialized | Model: DeepSeek-R1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent.parent\n",
        "load_dotenv(parent_dir / '.env')\n",
        "\n",
        "# Load environment\n",
        "load_dotenv()\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "model_name = os.getenv(\"MODEL_NAME\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key)\n",
        "    )\n",
        "    print(\"‚úÖ Client initialized | Model:\", model_name)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Plan a 5-day cultural trip to Kyoto in April\n",
            "\n",
            "üß† Thinking Process: Okay, the user wants a 5-day cultural trip to Kyoto in April, including hidden gems and safety tips. Let me start by recalling Kyoto's main attractions and the best times to visit them. April is cherry blossom season, so some spots might be crowded. They mentioned hidden gems, so I need to include less touristy places. Also, safety considerations are important, especially for travelers.\n",
            "\n",
            "First day: Arrival and initial exploration. Maybe start with Fushimi Inari Taisha since it's iconic. But the user wants hidden gems, so perhaps suggest going early to avoid crowds and explore the lesser-known trails there. Then maybe a traditional tea house in Gion, but not the main area. Safety tip: be cautious with taxis, use public transport.\n",
            "\n",
            "Second day: Focus on temples and gardens. Kinkaku-ji is a must, but it's crowded. Suggest Ryoan-ji nearby, which is a Zen garden and maybe less crowded. Then maybe a hidden temple like Honen-in. Lunch in a local spot, maybe a vegetarian place. Afternoon could be Philosopher's Path, which is popular but in April with cherry blossoms. Maybe add a lesser-known section or a nearby gallery. Safety: watch for slippery paths if it rains.\n",
            "\n",
            "Third day: Day trip to Arashiyama. Bamboo Grove is famous but busy. Go early. Then Tenryu-ji temple. Hidden gem: Okochi Sanso Villa nearby. Maybe a boat ride on the river. Safety: be cautious of pickpockets in crowded areas.\n",
            "\n",
            "Fourth day: Cultural workshops. Tea ceremony experience, maybe in a less touristy studio. Then Nishiki Market for lunch. Hidden gem: Pontocho Alley for dinner but suggest a specific small restaurant. Safety: food allergies, carry a card in Japanese.\n",
            "\n",
            "Fifth day: Departure. Last-minute shopping at Kyoto Station area. Maybe a hidden gem like a rooftop garden at the station. Safety: check train schedules in advance.\n",
            "\n",
            "Also, need to mention COVID-19 precautions, staying hydrated, and respecting etiquette at temples. Maybe include specific hidden cafes or shops. Check if the user has any mobility issues, but since not mentioned, general safety tips. Transportation passes like IC cards. Maybe recommend a ryokan stay for cultural experience.\n",
            "\n",
            "Wait, did I miss any hidden spots? Maybe the Tofuku-ji Temple's sub-temples, or the Shugakuin Imperial Villa which requires reservation. Also, the Gio-ji Temple near Arashiyama. For safety, maybe note that some hidden gems might have uneven paths, so wear comfortable shoes. Also, April weather can be variable, so packing layers and rain gear.\n",
            "\n",
            "I should structure each day with morning, afternoon, evening, and include rationale for each choice. Make sure the hidden gems are integrated into each day. Safety tips at the end of each day or in a separate section. Maybe a packing list as part of the safety considerations. Also, transportation between locations‚ÄîKyoto‚Äôs buses and trains. Maybe suggest renting a bike for some areas if the user is comfortable.\n",
            "\n",
            "Double-check the days to ensure logical flow and proximity of locations. Avoid backtracking. For example, Arashiyama is a bit out, so a full day there. Day 2 could be northern Kyoto temples. Day 4 in central Kyoto for workshops and markets. Also, check if any special events in April, like festivals, but maybe that's too crowded. Alternatively, suggest smaller local events.\n",
            "\n",
            "Need to mention cash usage, as some smaller places don't take cards. Emergency numbers. Maybe a map or app recommendation for navigation. Also, language tips, like basic Japanese phrases.\n",
            "\n",
            "Okay, structure the plan with each day broken down, hidden gems highlighted, and safety tips integrated. Add a section at the end with general safety and packing advice. Make sure the hidden gems are specific and not the usual spots. Maybe include places like the Kyoto Railway Museum if they like trains, but the user asked for cultural, so maybe not. Focus on temples, gardens, workshops, and local eateries.\n",
            "\n",
            "üìù Final Answer: **5-Day Cultural Trip to Kyoto in April: Hidden Gems & Safety Tips**  \n",
            "*Rationale: Balances iconic sights with lesser-known spots, leverages cherry blossoms (April), and prioritizes safety/local etiquette.*\n",
            "\n",
            "---\n",
            "\n",
            "### **Day 1: Arrival & Spiritual Immersion**  \n",
            "**Morning:**  \n",
            "- **Fushimi Inari Taisha** (6:00‚Äì8:00 AM): Beat crowds by arriving early. Explore hidden trails beyond the main torii gates (e.g., the bamboo-lined **Senbon Torii** path).  \n",
            "- **Breakfast:** Try *Inari sushi* at a stall near the shrine‚Äôs base.  \n",
            "\n",
            "**Afternoon:**  \n",
            "- **Tofuku-ji Temple**: Visit its sub-temples like **H≈çj≈ç Garden** (Zen rock garden) and **Kaizan-d≈ç Hall** (quiet, fewer tourists).  \n",
            "- **Lunch**: *War\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
            "Consider indexing strategies, hardware requirements, and query optimization.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, so I need to figure out how to optimize a PostgreSQL database that's handling 10k transactions per second. That's a pretty high load, so there's a lot to consider here. Let me start by breaking down the problem into parts. The user mentioned indexing strategies, hardware requirements, and query optimization. Maybe I should tackle each of these areas one by one.\n",
            "\n",
            "First, indexing. I know that indexes are crucial for speeding up queries, but they can also slow down writes because every insert, update, or delete has to update the index. So, for a high transaction rate, I need to make sure that indexes are optimized. Maybe using the right types of indexes. For example, B-tree indexes are the default in PostgreSQL and are good for a range of queries. But if there are a lot of columns used in WHERE clauses, maybe composite indexes would help. But then again, too many indexes can be a problem. So, the key is to index the right columns, maybe using tools like EXPLAIN ANALYZE to find missing indexes or redundant ones. Also, maybe considering partial indexes if there are queries that only touch a subset of data. Oh, and index bloat could be an issue with high write volumes, so regular maintenance like REINDEX or using pg_repack might be necessary.\n",
            "\n",
            "Next, hardware requirements. 10k transactions per second is a lot. So the hardware needs to handle that. I'm thinking about CPU, RAM, storage, and network. For CPU, PostgreSQL is pretty CPU-intensive, especially for complex queries. So multi-core processors would help, but PostgreSQL's concurrency model can sometimes be a bottleneck. Maybe using a connection pooler like pgbouncer to handle the connections more efficiently. For RAM, having enough to fit the working set into memory is important. The shared_buffers parameter in PostgreSQL should be set to a portion of the total RAM, maybe around 25% of available memory. But also, the OS cache is important, so not setting it too high. For storage, SSDs are a must for high IOPS. Maybe using RAID configurations or even NVMe drives for faster access. Also, considering the write-ahead log (WAL) configuration. Maybe putting WAL on a separate disk to reduce contention. Network-wise, having a high-throughput network interface to handle the traffic, maybe 10 GbE or higher.\n",
            "\n",
            "Then, query optimization. Even with good indexes and hardware, poorly written queries can kill performance. So, analyzing slow queries using pg_stat_statements to find the ones that take the most time or are called most frequently. Using EXPLAIN to see the query plans and look for sequential scans where indexes should be used, or inefficient joins. Maybe rewriting queries to be more efficient, using joins instead of subqueries, or avoiding SELECT *. Also, batch processing where possible, to reduce the number of round trips. Maybe using prepared statements to reduce parsing overhead. Also, looking into transaction isolation levels. If some transactions can use a lower isolation level, that might reduce locking and improve concurrency.\n",
            "\n",
            "Wait, partitioning could be another aspect. If the tables are very large, partitioning them by range or some key can help with query performance and maintenance. PostgreSQL has declarative partitioning now, which might help in managing large tables. Also, connection pooling is important because opening a new connection for each transaction is expensive. So using a connection pooler like pgbouncer in transaction mode to reuse connections and reduce overhead.\n",
            "\n",
            "Replication and read scaling might be necessary. If the workload is read-heavy, maybe setting up read replicas to distribute the load. But if it's write-heavy, then the primary server needs to handle those writes, and maybe using sharding. Although sharding in PostgreSQL isn't as straightforward as in some other databases. Tools like Citus might help for horizontal scaling.\n",
            "\n",
            "Configuration tuning. The PostgreSQL configuration file (postgresql.conf) has a lot of parameters that can be adjusted. For example, increasing max_connections (but better to use connection pooling instead of just increasing this), work_mem for sort and hash operations, maintenance_work_mem for vacuum operations, effective_cache_size to let the planner know how much memory is available. Also, checkpoint_segments and checkpoint_timeout to tune how often checkpoints occur, which can reduce I/O spikes. Autovacuum settings are critical too. With high write volumes, autovacuum needs to be aggressive enough to prevent bloat. Maybe lowering autovacuum_vacuum_cost_limit so that autovacuum runs more frequently without causing too much load.\n",
            "\n",
            "Monitoring is also important. Using tools like pg_stat_activity to monitor current queries, pg_stat_user_tables to see table access patterns, and setting up alerts for long-running transactions or locks. Maybe using tools like Prometheus with Grafana for dashboards.\n",
            "\n",
            "Another thing is considering the use of UNLOGGED tables for temporary data that can be recreated, as they reduce WAL overhead. But they get truncated on crash, so only for non-critical data.\n",
            "\n",
            "Also, maybe using materialized views for complex queries that are run frequently but don't need real-time data. Refreshing them periodically can reduce the load on the main tables.\n",
            "\n",
            "Wait, and what about locking? High concurrency can lead to lock contention. Using row-level locks where possible, or avoiding long-running transactions that hold locks. Maybe using the NOWAIT clause in SELECT FOR UPDATE to prevent waiting on locks.\n",
            "\n",
            "Let me try to structure this. For indexing strategies: use appropriate index types, partial indexes, composite indexes, regular index maintenance. For hardware: SSDs, sufficient RAM, multi-core CPUs, separate WAL disk, network bandwidth. Query optimization: analyze and tune slow queries, use prepared statements, batch operations, proper isolation levels. Configuration: adjust memory settings, autovacuum, checkpoints, connection pooling. Scaling: read replicas, partitioning, maybe sharding. Monitoring and maintenance with tools.\n",
            "\n",
            "I should also mention things like avoiding foreign key constraints if they're causing too much overhead, or deferring constraints. But that's a trade-off with data integrity.\n",
            "\n",
            "Another point is using the latest PostgreSQL version, as performance improvements are made in newer releases. Also, considering extensions like pg_partman for partition management, or timescaledb if dealing with time-series data.\n",
            "\n",
            "Wait, but the user didn't mention the type of data. Maybe it's general, but the answer should be general as well.\n",
            "\n",
            "So putting all together, the answer should cover these areas with specific strategies in each. Maybe bullet points under each category.\n",
            "</think>\n",
            "\n",
            "To optimize a PostgreSQL database handling **10k transactions/second**, implement the following strategies across indexing, hardware, query optimization, and configuration:\n",
            "\n",
            "### **1. Indexing Strategies**\n",
            "- **Targeted Indexes**: Use `EXPLAIN ANALYZE` to identify missing indexes. Prioritize columns in `WHERE`, `JOIN`, and `ORDER BY` clauses.\n",
            "- **Partial Indexes**: For queries filtering on a subset of data (e.g., `WHERE status = 'active'`).\n",
            "- **Composite Indexes**: Combine frequently queried columns to avoid multiple index scans.\n",
            "- **Index Maintenance**: Regularly rebuild bloated indexes with `REINDEX` or `pg_repack`.\n",
            "- **Avoid Over-Indexing**: Remove unused or redundant indexes to reduce write overhead.\n",
            "\n",
            "### **2. Hardware Optimization**\n",
            "- **Storage**: Use NVMe SSDs for high IOPS. Separate WAL and data onto dedicated disks.\n",
            "- **RAM**: Allocate 70-80% of total memory to PostgreSQL (split between `shared_buffers` and OS cache). Aim for ‚â•128GB RAM.\n",
            "- **CPU**: Multi-core processors (‚â•16 cores) to handle parallelism. Enable `parallel_workers` for large queries.\n",
            "- **Network**: 10 GbE+ to minimize latency between app/database layers.\n",
            "\n",
            "### **3. Query Optimization**\n",
            "- **Analyze Slow Queries**: Use `pg_stat_statements` to identify bottlenecks. Optimize full-table scans and inefficient joins.\n",
            "- **Batch Operations**: Reduce round trips with bulk inserts/updates.\n",
            "- **Prepared Statements**: Minimize parsing overhead for frequent queries.\n",
            "- **Avoid `SELECT *`**: Fetch only necessary columns.\n",
            "- **Materialized Views**: Cache complex query results for read-heavy workloads.\n",
            "\n",
            "### **4. Configuration Tuning**\n",
            "- **Memory Settings**:\n",
            "  - `work_mem`: 8-16MB for sorting/hashing.\n",
            "  - `maintenance_work_mem`: 1-2GB for vacuuming.\n",
            "  - `effective_cache_size`: ~75% of total RAM.\n",
            "- **Checkpoints**: Increase `checkpoint_timeout` (e.g., 30min) and `max_wal_size` to reduce I/O spikes.\n",
            "- **Autovacuum**: Tune `autovacuum_vacuum_scale_factor` and `autovacuum_vacuum_cost_limit` to prevent bloat.\n",
            "- **Connection Pooling**: Use `pgbouncer` (transaction mode) to limit idle connections and reduce overhead.\n",
            "\n",
            "### **5. Scaling & Architecture**\n",
            "- **Read Replicas**: Offload read traffic using streaming replication.\n",
            "- **Partitioning**: Split large tables by time or key (e.g., PostgreSQL declarative partitioning).\n",
            "- **Sharding**: For write-heavy workloads, use Citus or application-level sharding.\n",
            "\n",
            "### **6. Monitoring & Maintenance**\n",
            "- **Metrics**: Track locks, replication lag, and buffer cache hit ratio via `pg_stat_activity`, `pg_stat_replication`.\n",
            "- **Alerting**: Set thresholds for long-running transactions, table bloat, and connection limits.\n",
            "- **Logging**: Enable `log_min_duration_statement` to log slow queries.\n",
            "\n",
            "### **7. Advanced Techniques**\n",
            "- **UNLOGGED Tables**: For transient data (e.g., sessions) to reduce WAL writes.\n",
            "- **Lock Management**: Use `NOWAIT\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai-foundry-workshop",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
