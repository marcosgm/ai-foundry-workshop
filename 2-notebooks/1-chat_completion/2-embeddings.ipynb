{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3e8f66",
   "metadata": {},
   "source": [
    "# üèãÔ∏è Fun with Text and Image Embeddings üçé\n",
    "\n",
    "Welcome to our **Health & Fitness** embeddings notebook! In this tutorial, we'll show you how to:\n",
    "\n",
    "1. **Initialize** an `AIProjectClient` to access your Azure AI Foundry project.\n",
    "2. **Embed text** using `azure-ai-inference` with our fun health-themed phrases.\n",
    "3. **Embed images** using `azure-ai-inference` (we're using Cohere's image embeddings model).\n",
    "4. **Generate a health-themed image** (example code) and display it.\n",
    "5. **Use a prompt template** for extra context.\n",
    "\n",
    "Let's get started and have some fun with our healthy ideas! üçè\n",
    "\n",
    "> **Disclaimer**: This notebook is for educational purposes only. Always consult a professional for medical advice.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-embeddings.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd2ac6",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment\n",
    "\n",
    "#### Prerequisites:\n",
    "- Deploy a text embeddings model (**text-embedding-3-small**) in Azure AI Foundry\n",
    "- Deploy a image embeddings model (**Cohere-embed-v3-english**) in Azure AI Foundry\n",
    "#\n",
    "We'll import our libraries and load the environment variables for:\n",
    "- `PROJECT_CONNECTION_STRING`: Your project connection string.\n",
    "- `TEXT_EMBEDDING_MODEL`: The text embeddings model deployment name.\n",
    "- `IMAGE_EMBEDDING_MODEL`: The image embeddings model deployment name.\n",
    "\n",
    "We'll import libraries, load environment variables, and create an `AIProjectClient`.\n",
    "\n",
    "> #### Complete [1-basic-chat-completion.ipynb](./1-basic-chat-completion.ipynb) notebook before starting this one\n",
    "Let's begin! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50899c96",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "\n",
    "# Load environment variables\n",
    "notebook_path = Path().absolute()\n",
    "parent_dir = notebook_path.parent.parent\n",
    "load_dotenv(parent_dir / '.env')\n",
    "\n",
    "# Retrieve from environment or fallback\n",
    "PROJECT_CONNECTION_STRING = os.environ.get(\"PROJECT_CONNECTION_STRING\", \"<your-connection-string>\")\n",
    "TEXT_EMBEDDING_MODEL = os.environ.get(\"TEXT_EMBEDDING_MODEL\", \"text-embedding-3-large\")\n",
    "IMAGE_EMBEDDING_MODEL = os.environ.get(\"IMAGE_EMBEDDING_MODEL\", \"Cohere-embed-v3-english\")\n",
    "\n",
    "# Initialize project client\n",
    "try:\n",
    "    project_client = AIProjectClient.from_connection_string(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        conn_str=PROJECT_CONNECTION_STRING,\n",
    "    )\n",
    "    print(\"üéâ Successfully created AIProjectClient\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error creating AIProjectClient:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970dcce",
   "metadata": {},
   "source": [
    "## 2. Text Embeddings\n",
    "\n",
    "We'll call `get_embeddings_client()` from our `AIProjectClient` to retrieve the embeddings client. Then we'll embed some fun health-themed phrases:\n",
    "\n",
    "- \"üçé An apple a day keeps the doctor away\"\n",
    "- \"üèãÔ∏è 15-minute HIIT workout routine\"\n",
    "- \"üßò Mindful breathing exercises\"\n",
    "\n",
    "The output will be numeric vectors representing each phrase in semantic space. Let‚Äôs see those embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28466a95",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "text_phrases = [\n",
    "    \"An apple a day keeps the doctor away üçé\",\n",
    "    \"Quick 15-minute HIIT workout routine üèãÔ∏è\",\n",
    "    \"Mindful breathing exercises üßò\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with project_client.inference.get_embeddings_client() as embed_client:\n",
    "        response = embed_client.embed(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=text_phrases\n",
    "        )\n",
    "\n",
    "        for item in response.data:\n",
    "            vec = item.embedding\n",
    "            sample_str = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "            print(f\"Sentence {item.index}: '{text_phrases[item.index]}':\\n\" \\\n",
    "                  f\"  Embedding length={len(vec)}\\n\" \\\n",
    "                  f\"  Sample: {sample_str}\\n\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding text:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b0402",
   "metadata": {},
   "source": [
    "## 3. Prompt Template Example üìù\n",
    "\n",
    "Even though our focus is on embeddings, here's how you might prepend some context to a user message. Imagine you want to embed user text but first add a system prompt such as ‚ÄúYou are HealthFitGPT, a fitness guidance model‚Ä¶‚Äù This little extra helps set the stage for more context-aware embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3e392",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "# A basic prompt template (system-style) we'll prepend to user text.\n",
    "TEMPLATE_SYSTEM = (\n",
    "    \"You are HealthFitGPT, a fitness guidance model.\\n\"\n",
    "    \"Please focus on healthy advice and disclaim you're not a doctor.\\n\\n\"\n",
    "    \"User message:\"  # We'll append the user message after this.\n",
    ")\n",
    "\n",
    "def embed_with_template(user_text):\n",
    "    \"\"\"Embed user text with a system template in front.\"\"\"\n",
    "    content = TEMPLATE_SYSTEM + \" \" + user_text\n",
    "    with project_client.inference.get_embeddings_client() as embed_client:\n",
    "        rsp = embed_client.embed(\n",
    "            model=TEXT_EMBEDDING_MODEL,\n",
    "            input=[content]\n",
    "        )\n",
    "    return rsp.data[0].embedding\n",
    "\n",
    "sample_user_text = \"Can you suggest a quick home workout for busy moms?\"\n",
    "embedding_result = embed_with_template(sample_user_text)\n",
    "print(\"Embedding length:\", len(embedding_result))\n",
    "print(\"First few dims:\", embedding_result[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571c972",
   "metadata": {},
   "source": [
    "## 4. Image Embeddings\n",
    "\n",
    "Image embeddings typically require managed compute resources. While Azure AI Foundry offers specialized models (like **MedImageInsight**) for medical images, in this example we'll use Cohere's serverless image embedding model.\n",
    "\n",
    "Here we are using the **`hand-xray.png`** image to generate embeddings. This image (of a hand X-ray) is our fun nod to health-themed imagery!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bbc40",
   "metadata": {
    "executionInfo": {}
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with project_client.inference.get_image_embeddings_client() as img_embed_client:\n",
    "        # Construct input for the image embeddings call\n",
    "        img_input = ImageEmbeddingInput.load(image_file=\"hand-xray.png\", image_format=\"png\")\n",
    "        resp = img_embed_client.embed(\n",
    "            model=IMAGE_EMBEDDING_MODEL,\n",
    "            input=[img_input]\n",
    "        )\n",
    "\n",
    "        for item in resp.data:\n",
    "            vec = item.embedding\n",
    "            snippet = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "            print(f\"Image index={item.index}, length={len(vec)}, sample={snippet}\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error embedding image:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6dece",
   "metadata": {},
   "source": [
    "## 5. Wrap-Up & Next Steps\n",
    "üéâ We've shown how to:\n",
    "- Set up the `AIProjectClient`.\n",
    "- Get **text embeddings** using *text-embedding-3-small*.\n",
    "- Get **image embeddings** using *Cohere-embed-v3-english* on a health-themed (hand X-ray) image.\n",
    "- **Generate** a health-themed image (example code).\n",
    "- Use a **prompt template** to add system context to your embeddings.\n",
    "\n",
    "**Where to go next?**\n",
    "- Explore `azure-ai-evaluation` for evaluating your embeddings.\n",
    "- Use `azure-core-tracing-opentelemetry` for end-to-end telemetry.\n",
    "- Build out a retrieval pipeline to compare similarity of embeddings.\n",
    "\n",
    "Have fun experimenting, and remember: when it comes to your health, always consult a professional!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
